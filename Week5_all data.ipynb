{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5917c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as func\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from models import VGG\n",
    "from train import train1Epoch\n",
    "from train import test1Epoch\n",
    "from datasetCreator import ImageSubset\n",
    "torch.cuda.empty_cache()\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3f03289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'runs': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm -v -f runs\\*\n",
    "!rm -r runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97cd736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00a8a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "#writer=SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15a50f03",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1158549507.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3581/1158549507.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ssh -N -f -L localhost:16006:localhost:6006 ddavilag@dsmlp-login.ucsd.edu\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ssh -N -f -L localhost:16006:localhost:6006 ddavilag@dsmlp-login.ucsd.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0be7840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: path: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir <path> --port 16006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6156dd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2a970fba4d5a64da\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2a970fba4d5a64da\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/43837372/how-does-one-open-a-tensorboard-port-in-linux\n",
    "%tensorboard --logdir <path-to-your-logs> --path_prefix /tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cc9b625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/xdg-open: 869: www-browser: not found\r\n",
      "/usr/bin/xdg-open: 869: links2: not found\r\n",
      "/usr/bin/xdg-open: 869: elinks: not found\r\n",
      "/usr/bin/xdg-open: 869: links: not found\r\n",
      "/usr/bin/xdg-open: 869: lynx: not found\r\n",
      "/usr/bin/xdg-open: 869: w3m: not found\r\n",
      "xdg-open: no method available for opening 'http://localhost:6006'\r\n"
     ]
    }
   ],
   "source": [
    "#!start chrome http://localhost:6006/\n",
    "!xdg-open http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86af156",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00cec9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, '11.1')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install https://download.pytorch.org/whl/nightly/cu116/torch-1.14.0.dev20221023%2Bcu116-cp39-cp39-win_amd64.whl\n",
    "#pip install https://download.pytorch.org/whl/nightly/cu116/torchvision-0.15.0.dev20221023%2Bcu116-cp39-cp39-win_amd64.whl\n",
    "torch.cuda.is_available(), torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dd594a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/ddavilag/teams/dsc-180a---a14-[88137]/df_bnpp_datapaths.csv'\n",
    "KEY_PATH = '/home/ddavilag/teams/dsc-180a---a14-[88137]/df_bnpp_keys.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e5145c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datapaths = pd.read_csv(DATA_PATH, header=None).T.merge(pd.read_csv(KEY_PATH, header=None).T, left_index=True, right_index=True)\n",
    "df_datapaths.columns = ['filepaths', 'key']\n",
    "df_datapaths.key = df_datapaths.key.apply(lambda x: eval(x))\n",
    "df_datapaths.filepaths = df_datapaths.filepaths.apply(lambda x: eval(x))\n",
    "df_datapaths = df_datapaths.set_index('key')\n",
    "#missing h5py files 7-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "767f6a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21374, 2) (2602, 2) (2692, 2)\n"
     ]
    }
   ],
   "source": [
    "cols= ['unique_key', 'bnpp_value_log', 'BNP_value' \n",
    "        #'BNPP_weight', 'PNA_mask', 'PNA_wight_mask', 'BNP_value', 'age_at_sampletime'\n",
    "       ]\n",
    "test_df = pd.read_csv('/home/ddavilag/teams/dsc-180a---a14-[88137]/BNPP_DT_test_with_ages.csv', usecols = cols).set_index('unique_key')\n",
    "train_df = pd.read_csv('/home/ddavilag/teams/dsc-180a---a14-[88137]/BNPP_DT_train_with_ages.csv', usecols = cols).set_index('unique_key')\n",
    "val_df = pd.read_csv('/home/ddavilag/teams/dsc-180a---a14-[88137]/BNPP_DT_val_with_ages.csv', usecols = cols).set_index('unique_key')\n",
    "print(train_df.shape, test_df.shape, val_df.shape)\n",
    "train_df['heart'] = train_df['BNP_value'].apply(lambda x: int(x > 400))\n",
    "test_df['heart'] = test_df['BNP_value'].apply(lambda x: int(x > 400))\n",
    "val_df['heart'] = val_df['BNP_value'].apply(lambda x: int(x > 400))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd24f5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15164, 4) (1823, 4) (1913, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.sort_index().merge(df_datapaths, left_index=True, right_index=True)\n",
    "test_df = test_df.sort_index().merge(df_datapaths, left_index=True, right_index=True)\n",
    "val_df = val_df.sort_index().merge(df_datapaths, left_index=True, right_index=True)\n",
    "print(train_df.shape, test_df.shape, val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "115789e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15164, 4), (1823, 4), (1913, 4), (3736, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['filepaths'] = train_df['filepaths'].str.replace('jmryan','ddavilag')\n",
    "test_df['filepaths'] = test_df['filepaths'].str.replace('jmryan','ddavilag')\n",
    "val_df['filepaths'] = val_df['filepaths'].str.replace('jmryan','ddavilag')\n",
    "new_valid = pd.concat([test_df,val_df],ignore_index=False)\n",
    "train_df.shape, test_df.shape, val_df.shape,new_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00abbb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18900, 3736)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15164+1823+1913, 1823+1913\n",
    "#number of rows that we currently have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42f5c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(names='unique_key',inplace=True)\n",
    "new_valid.reset_index(names='unique_key',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f032e005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_key</th>\n",
       "      <th>bnpp_value_log</th>\n",
       "      <th>BNP_value</th>\n",
       "      <th>heart</th>\n",
       "      <th>filepaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abachug_50267230_img1</td>\n",
       "      <td>2.621176</td>\n",
       "      <td>418.0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/ddavilag/teams/dsc-180a---a14-[88137]/bn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abafouck_52403307_img1</td>\n",
       "      <td>2.071882</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ddavilag/teams/dsc-180a---a14-[88137]/bn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abagash_52691625_img1</td>\n",
       "      <td>1.698101</td>\n",
       "      <td>49.9</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ddavilag/teams/dsc-180a---a14-[88137]/bn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abakleem_50725934_img1</td>\n",
       "      <td>4.301659</td>\n",
       "      <td>20029.0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/ddavilag/teams/dsc-180a---a14-[88137]/bn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abaphos_51936331_img1</td>\n",
       "      <td>3.219323</td>\n",
       "      <td>1657.0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/ddavilag/teams/dsc-180a---a14-[88137]/bn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               unique_key  bnpp_value_log  BNP_value  heart  \\\n",
       "0   Abachug_50267230_img1        2.621176      418.0      1   \n",
       "1  Abafouck_52403307_img1        2.071882      118.0      0   \n",
       "2   Abagash_52691625_img1        1.698101       49.9      0   \n",
       "3  Abakleem_50725934_img1        4.301659    20029.0      1   \n",
       "4   Abaphos_51936331_img1        3.219323     1657.0      1   \n",
       "\n",
       "                                           filepaths  \n",
       "0  /home/ddavilag/teams/dsc-180a---a14-[88137]/bn...  \n",
       "1  /home/ddavilag/teams/dsc-180a---a14-[88137]/bn...  \n",
       "2  /home/ddavilag/teams/dsc-180a---a14-[88137]/bn...  \n",
       "3  /home/ddavilag/teams/dsc-180a---a14-[88137]/bn...  \n",
       "4  /home/ddavilag/teams/dsc-180a---a14-[88137]/bn...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "345b9698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_key</th>\n",
       "      <th>bnpp_value_log</th>\n",
       "      <th>BNP_value</th>\n",
       "      <th>heart</th>\n",
       "      <th>filepaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abadik_50217497_img1</td>\n",
       "      <td>3.334655</td>\n",
       "      <td>2161.0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/ddavilag/teams/dsc-180a---a14-[88137]/bn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abdisey_52768446_img1</td>\n",
       "      <td>2.451786</td>\n",
       "      <td>283.0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ddavilag/teams/dsc-180a---a14-[88137]/bn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abenog_51901728_img1</td>\n",
       "      <td>2.647383</td>\n",
       "      <td>444.0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/ddavilag/teams/dsc-180a---a14-[88137]/bn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abeyned_51238065_img1</td>\n",
       "      <td>2.869232</td>\n",
       "      <td>740.0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/ddavilag/teams/dsc-180a---a14-[88137]/bn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abibey_51056225_img1</td>\n",
       "      <td>2.710117</td>\n",
       "      <td>513.0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/ddavilag/teams/dsc-180a---a14-[88137]/bn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              unique_key  bnpp_value_log  BNP_value  heart  \\\n",
       "0   Abadik_50217497_img1        3.334655     2161.0      1   \n",
       "1  Abdisey_52768446_img1        2.451786      283.0      0   \n",
       "2   Abenog_51901728_img1        2.647383      444.0      1   \n",
       "3  Abeyned_51238065_img1        2.869232      740.0      1   \n",
       "4   Abibey_51056225_img1        2.710117      513.0      1   \n",
       "\n",
       "                                           filepaths  \n",
       "0  /home/ddavilag/teams/dsc-180a---a14-[88137]/bn...  \n",
       "1  /home/ddavilag/teams/dsc-180a---a14-[88137]/bn...  \n",
       "2  /home/ddavilag/teams/dsc-180a---a14-[88137]/bn...  \n",
       "3  /home/ddavilag/teams/dsc-180a---a14-[88137]/bn...  \n",
       "4  /home/ddavilag/teams/dsc-180a---a14-[88137]/bn...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "622125a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessedImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, target_transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx,:]\n",
    "        filepath = row[4]  #image location\n",
    "        val = row[1] #bnpp value log\n",
    "        heart = row[3] #threshold for edema\n",
    "        \n",
    "        im = torch.load(filepath)\n",
    "        #print(filepath, val, heart)\n",
    "        #plt.imshow(im,cmap='gray')\n",
    "        #plt.show()\n",
    "        return im.view(1, 224, 224).expand(3, -1, -1), val#, heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d5282cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #maybe change to 16, 8\n",
    "num_workers = 2\n",
    "pin_memory = True\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.5), std=(0.5))\n",
    "#     ])\n",
    "\n",
    "train_set = PreprocessedImageDataset(df=train_df) #, transform = train_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "# valid_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.5), std=(0.5))\n",
    "#     ])\n",
    "\n",
    "valid_set = PreprocessedImageDataset(df=new_valid) #, transform = valid_transform)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "#print(train_set[0])\n",
    "#print(iter(train_loader).next())\n",
    "#print(train.__getitem__(0))\n",
    "#len(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e15e5d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pin_memory is True\n",
      "Finish with:67.97687101364136 second, num_workers=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish with:58.32607102394104 second, num_workers=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish with:32.53528666496277 second, num_workers=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 954, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 892, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 289, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/connection.py\", line 513, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/connection.py\", line 757, in answer_challenge\n",
      "    message = connection.recv_bytes(256)         # reject large message\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 4197) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 4197) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3581/965917230.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 4197) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import time\n",
    "pin_memory = True\n",
    "print('pin_memory is', pin_memory)\n",
    " \n",
    "for num_workers in range(0, 5, 1): \n",
    "    #train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    #sampler=train_sampler, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    start = time.time()\n",
    "    for epoch in range(1, 5):\n",
    "        for i, data in enumerate(train_loader):\n",
    "            pass\n",
    "    end = time.time()\n",
    "    print(\"Finish with:{} second, num_workers={}\".format(end - start, num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30987168",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0], valid_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448065d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(train_set), len(valid_loader),len(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_set[0][0].permute(1, 2, 0),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa199a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(valid_set[0][0].permute(1, 2, 0),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0bf204",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "model = VGG('VGG16').to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eeb77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys_r\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid)\n",
    "print('  '.join(str('%.2f' % labels[j].item()) for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"[INFO]: Computation device: {device}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"[INFO]: {total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"[INFO]: {total_trainable_params:,} trainable parameters.\")\n",
    "\n",
    "# the loss function, Mean Absolute Error\n",
    "loss_fn = nn.L1Loss().to(device)\n",
    "#loss_fn = nn.HuberLoss().to(device)\n",
    "#loss_fn = nn.SmoothL1Loss().to(device)\n",
    "\n",
    "# the optimizer\n",
    "# need to tune optimizer\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "LR = 0.0001\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=LR, rho=0.99)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.0001, amsgrad=True, weight_decay=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "#timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(f'runs/VGG16_Ada{LR}_batch{batch_size}_nodes{}')\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 25\n",
    "#best_vloss = 1_000_000.\n",
    "\n",
    "# the scheduler\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "tlosses, vlosses=[],[]\n",
    "writer.add_text('model params', \\\n",
    "                f'''VGG16\\n\n",
    "                loss: {'MAE'}\\n\n",
    "                optimizer: {'Ada'}\\n\n",
    "                \\tlearn rate: {LR}\\n\n",
    "                \\trho: {RHO}\\n\n",
    "                batch size: {batch_size}\\n\n",
    "                nodes: {500}\\n\n",
    "                dropout: {0.5}\\n\n",
    "                batchnorm: {'yes'}\\n\n",
    "                RELU: {'Leaky'}\\n\n",
    "                epochs: {EPOCHS}\n",
    "                ''',\\\n",
    "                0)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    #model.train(True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    avg_tloss = train1Epoch(epoch_number, model, optimizer, loss_fn, train_loader)\n",
    "    \n",
    "    #print(torch.cuda.memory_summary())\n",
    "    # We don't need gradients on to do reporting\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    #model.train(False)\n",
    "    with torch.no_grad():\n",
    "        avg_vloss = test1Epoch(epoch_number, model, loss_fn, valid_loader)\n",
    "\n",
    "    #print(torch.cuda.memory_summary())\n",
    "    print('LOSS train {} valid {}'.format(avg_tloss, avg_vloss))\n",
    "    writer.add_scalars('Loss', {'train':avg_tloss,'test':avg_vloss}, epoch)\n",
    "    tlosses.append(avg_tloss)\n",
    "    vlosses.append(avg_vloss)\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "#     if avg_vloss < best_vloss:\n",
    "#         best_vloss = avg_vloss\n",
    "#         model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n",
    "    #scheduler.step(avg_vloss)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd17046",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1,EPOCHS+1)\n",
    "df = pd.DataFrame(data={'train loss': tlosses,'valid loss':vlosses})\n",
    "#display(df)\n",
    "sns.set(style='whitegrid')\n",
    "#plt.figure(figsize=(20,3))\n",
    "\n",
    "g = sns.FacetGrid(df, height = 6)\n",
    "\n",
    "g = g.map(sns.lineplot, x=epochs,y=tlosses,marker='o', label='train')\n",
    "g = g.map(sns.lineplot, x=epochs,y=vlosses,color='red',marker='o', label='valid')\n",
    "g.set(ylim=(0, None))\n",
    "g.add_legend()\n",
    "plt.xticks(epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1,EPOCHS+1)\n",
    "df = pd.DataFrame(data={'train loss': tlosses,'valid loss':vlosses})\n",
    "#display(df)\n",
    "sns.set(style='whitegrid')\n",
    "#plt.figure(figsize=(20,3))\n",
    "\n",
    "g = sns.FacetGrid(df, height = 6)\n",
    "\n",
    "g = g.map(sns.lineplot, x=epochs,y=tlosses,marker='o', label='train')\n",
    "g = g.map(sns.lineplot, x=epochs,y=vlosses,color='red',marker='o', label='valid')\n",
    "g.set(ylim=(0, None))\n",
    "g.add_legend()\n",
    "plt.xticks(epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb4b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
